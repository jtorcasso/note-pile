{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Framework\n",
      "=========\n",
      "\n",
      "Normal linear regression where the model, $M$, is also a random variable. We are interested in the posterior distribution of $M$. For an outcome $Y$ we have the following conditional distribution:\n",
      "\n",
      "$Y|X_m,M,\\theta_m \\sim \\mathcal{N}(X_m\\beta_m,\\sigma_m^2)$\n",
      "\n",
      ", where $X_m$ is a subvector of the full set of predictors $X$ and $\\theta_m = (\\beta_m, \\sigma_m)$ are parameters for model $M=m$, such that\n",
      "$\\beta_m$ are regression coefficients on $X_m$ and $\\sigma_m^2$ is the variance.\n",
      "\n",
      "The posterior distribution of $M$ is as follows:\n",
      "\n",
      "$$\n",
      "Pr(M=m|Y,X) = \\frac{Pr(Y|X_m,M)Pr(M=m|X)}{\\sum_{m' \\in \\mathcal{M}}Pr(Y|X_{m'},M=m')Pr(M=m'|X)}\n",
      "$$\n",
      "\n",
      "\n",
      ", where $Pr(Y|X_m,M)$ is the likelihood and $Pr(M=m|X)$ is the prior probability of a particular model $m$. The likelihood is an *integrated* likelihood:\n",
      "\n",
      "$$\n",
      "Pr(Y|X_m,M) = \\int_{\\Theta_m}Pr(Y|X_m,M,\\theta_m)Pr(\\theta_m|X_m,M)d\\theta_m\n",
      "$$\n",
      "\n",
      ", which requires a prior on the parameters $Pr(\\theta_m|X_m,M)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting Distributions and Empirical Simplifications\n",
      "===================================================\n",
      "\n",
      "Likelihood\n",
      "----------\n",
      "Instead of estimating the *integrated* likelihood we choose a simple approximation, which uses the bayesian information criterion (BIC).\n",
      "\n",
      "$Pr(Y|X_m,M) = exp(\\frac{1}{2}BIC_m)$\n",
      "\n",
      ", where \n",
      "\n",
      "$BIC_m \\approx logPr(Y|X_m,M,\\hat{\\theta}_m) - \\frac{|\\theta_m|}{2}log(n)$\n",
      "\n",
      ", where\n",
      "\n",
      "$logPr(Y|X_m,M,\\hat{\\theta}_m) = \\sum_{i=1}^nlog\\phi_i(Y_i)$\n",
      "\n",
      ", where each $\\phi$ is a normal pdf with mean $X_{m,i}\\hat{\\beta}_m$ and \n",
      "variance $\\hat{\\sigma}_m^2$. These parameters are the typical OLS estimates. Hence, by using the BIC, we do not specify the prior for the parameters, because the error in the approximation includes the prior.\n",
      "\n",
      "Model Prior\n",
      "-----------\n",
      "The prior is improper and takes the following form:\n",
      "\n",
      "$Pr(M=m|X) = det(R_{X_m})\\prod_{k=1}^K\\pi_k^{\\delta_k}(1-\\pi_k)^{1-\\delta_k}$\n",
      "\n",
      ", where $\\delta_k=1$ if regressor $k$ is in $X_m$ (0 otherwise), $\\pi_k$ is a hyperparameter (we will set to 0.5) which denotes the prior probability of including a variable in the model. Thus, our prior is simply:\n",
      "\n",
      "$Pr(M=m|X) = det(R_{X_m})0.5^K$\n",
      "\n",
      "Estimation of Posterior Model Probability\n",
      "-----------------------------------------\n",
      "The posterior is estimated up to some proportionality constant. Otherwise, we would have to 1) estimate all of the models in $\\mathcal{M}$ and 2) specify a proper model prior.\n",
      "\n",
      "$Pr(M=m|Y,X) \\propto Pr(Y|X_m,M)Pr(M=m|X)$\n",
      "\n",
      "Setup for Algorithm\n",
      "-------------------\n",
      "\n",
      "1. Simulate data\n",
      "2. Set number of feasible predictors (this will restrict the state space, i.e. the models we can pick from)\n",
      "\n",
      "Steps for MCMC\n",
      "--------------\n",
      "\n",
      "1. Draw a model at random $m \\sim Unif(\\mathcal{M})$\n",
      "2. Propose new model $m'$ from a neighborhood of $m$, $nbd(m)$, \n",
      "    where $nbd(m)$ is defined by the set of models with either\n",
      "    1 more or 1 less regressor (while still satisfying the \n",
      "    feasible number of predictors constraint)\n",
      "3. Calculate (up to a proportionality constant) the posterior\n",
      "    probabilities of $m$ and $m'$. \n",
      "    \n",
      "4. Move to $m'$ with probability = $max\\{1, Pr(m'|X,Y)/Pr(m|X,Y)\\}\n",
      "\n",
      "5. Repeat steps 2 through 4 until number of iterations is satisfied"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using PyMC to Sample from the Posterior Model Distribution\n",
      "==========================================================\n",
      "\n",
      "\n",
      "\n",
      "Example with Simulated Data\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "from pymc import stochastic, DiscreteMetropolis, MCMC\n",
      "import statsmodels.api as sm\n",
      "import pandas as pd\n",
      "import random\n",
      "\n",
      "def pack(alist, rank):\n",
      "    \n",
      "    binary = [str(1) if i in alist else str(0) for i in xrange(0,rank)]\n",
      "    string = '0b1'+''.join(binary)\n",
      "    return int(string, 2)\n",
      "\n",
      "def unpack(integer):\n",
      "    \n",
      "    string = bin(integer)[3:]\n",
      "    \n",
      "    return [int(i) for i in xrange(len(string)) if string[i]=='1']\n",
      "\n",
      "def make_bma():\n",
      "    \n",
      "    # Simulating Data\n",
      "    size = 100\n",
      "    rank = 20  \n",
      "    \n",
      "    X = 10*np.random.randn(size, rank)\n",
      "    error = 30*np.random.randn(size,1)\n",
      "    coefficients = np.array([10, 2, 2, 2, 2, 2]).reshape((6,1))\n",
      "    y = np.dot(sm.add_constant(X[:,:5], prepend=True), coefficients) + error\n",
      "    \n",
      "    # Number of allowed regressors    \n",
      "    predictors = [3,4,5,6,7]\n",
      "    \n",
      "    @stochastic(dtype=int)\n",
      "    def regression_model():\n",
      "        \n",
      "        def logp(value):\n",
      "\n",
      "            columns = unpack(value)\n",
      "            \n",
      "            x = sm.add_constant(X[:,columns], prepend=True)\n",
      "            \n",
      "            corr = np.corrcoef(x[:,1:], rowvar=0)\n",
      "        \n",
      "            prior = np.linalg.det(corr)\n",
      "    \n",
      "            ols = sm.OLS(y,x).fit()\n",
      "            \n",
      "            posterior = np.exp(-0.5*ols.bic)*prior\n",
      "    \n",
      "            return np.log(posterior)\n",
      "    \n",
      "        def random():\n",
      "            \n",
      "            k = np.random.choice(predictors)\n",
      "            \n",
      "            columns = sorted(np.random.choice(xrange(0,rank), size=k, replace=False))\n",
      "            \n",
      "            return pack(columns, rank)\n",
      "    \n",
      "    class ModelMetropolis(DiscreteMetropolis):\n",
      "        def __init__(self, stochastic):\n",
      "            DiscreteMetropolis.__init__(self, stochastic)\n",
      "            \n",
      "        def propose(self):\n",
      "            '''considers a neighborhood around the previous model, \n",
      "            defined as having one regressor removed or added, provided\n",
      "            the total number of regressors coincides with predictors.\n",
      "            \n",
      "            Draws uniformly from this neighborhood to form proposal.\n",
      "            '''\n",
      "            \n",
      "            # Building set of neighboring models\n",
      "            last = unpack(self.stochastic.value)\n",
      "            last_indicator = np.zeros(rank)\n",
      "            last_indicator[last] = 1\n",
      "            last_indicator = last_indicator.reshape((-1,1))\n",
      "            neighbors = abs(np.diag(np.ones(rank)) - last_indicator)\n",
      "            neighbors = neighbors[:,np.any([neighbors.sum(axis=0) == i \\\n",
      "                                for i in predictors], axis=0)]\n",
      "            neighbors = pd.DataFrame(neighbors)\n",
      "            \n",
      "            # Drawing one model at random from the neighborhood\n",
      "            draw = random.choice(xrange(neighbors.shape[1]))\n",
      "    \n",
      "            self.stochastic.value = pack(list(neighbors[draw][neighbors[draw]==1].index), rank)\n",
      "        \n",
      "#        def step(self):\n",
      "#            \n",
      "#            logp_p = self.stochastic.logp\n",
      "#            \n",
      "#            self.propose()\n",
      "#            \n",
      "#            logp = self.stochastic.logp\n",
      "#            \n",
      "#            if np.log(random.random()) > logp_p - logp:\n",
      "#                \n",
      "#                self.reject()\n",
      "                \n",
      "            \n",
      "            \n",
      "            \n",
      "    \n",
      "    return locals()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = make_bma()\n",
      "M = MCMC(model)\n",
      "M.use_step_method(model['ModelMetropolis'], model['regression_model'])\n",
      "M.sample(iter=5000, burn=1000, thin=1)\n",
      "\n",
      "model_trace = M.trace(\"regression_model\")[:]\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "counts = Counter(model_trace).items()\n",
      "counts.sort(reverse=True, key=lambda x: x[1])\n",
      "\n",
      "print(\" \")\n",
      "for f in counts[:10]:\n",
      "    columns = unpack(f[0])\n",
      "    print('Visits:', f[1])\n",
      "    print(np.array([1. if i in columns else 0 for i in range(0,M.rank)]))\n",
      "    print(M.coefficients.flatten())\n",
      "    X = sm.add_constant(M.X[:, columns], prepend=True)\n",
      "    corr = np.corrcoef(X[:,1:], rowvar=0)\n",
      "    prior = np.linalg.det(corr)\n",
      "    fit = sm.OLS(model['y'],X).fit()\n",
      "    posterior = np.exp(-0.5*fit.bic)*prior\n",
      "    print(fit.params)\n",
      "    print('R-squared:', fit.rsquared)\n",
      "    print('BIC', fit.bic)\n",
      "    print('Prior', prior)\n",
      "    print('Posterior', posterior)\n",
      "    print(\" \")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r",
        "[****************100%******************]  5000 of 5000 complete"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "Visits: 4\n",
        "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "  1.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 19.04273544  -0.18947073   0.48886034  -0.8969503 ]\n",
        "R-squared: 0.0457413282667\n",
        "BIC 1077.24745427\n",
        "Prior 0.95299689286\n",
        "Posterior 1.14229682212e-234\n",
        " \n",
        "Visits: 4\n",
        "[ 0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 15.3377702    1.85734763   1.96868626  -0.14939391]\n",
        "R-squared: 0.221738991355\n",
        "BIC 1056.86017182\n",
        "Prior 0.977934116567\n",
        "Posterior 3.13356866e-230\n",
        " \n",
        "Visits: 4\n",
        "[ 0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 18.3614317    2.37348395   1.60309694  -0.13676573]\n",
        "R-squared: 0.429812810233\n",
        "BIC 1025.75044736\n",
        "Prior 0.977779614164\n",
        "Posterior 1.78386271813e-223\n",
        " \n",
        "Visits: 4\n",
        "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 17.27622165   1.54992855   0.41036155  -0.49260651]\n",
        "R-squared: 0.0981059600433\n",
        "BIC 1071.60368044\n",
        "Prior 0.991884894677\n",
        "Posterior 1.99838276193e-233\n",
        " \n",
        "Visits: 3\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 19.19951857  -0.44450139  -0.53813808  -0.64773941]\n",
        "R-squared: 0.0337205267587\n",
        "BIC 1078.49928657\n",
        "Prior 0.976083345777\n",
        "Posterior 6.25665855716e-235\n",
        " \n",
        "Visits: 3\n",
        "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "  1.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 18.4711597    1.56952938  -0.54538789  -0.93378544]\n",
        "R-squared: 0.13294289972\n",
        "BIC 1067.66445978\n",
        "Prior 0.997784589087\n",
        "Posterior 1.44093731556e-232\n",
        " \n",
        "Visits: 3\n",
        "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 18.29865117  -0.58204743   0.21638424  -0.31528488]\n",
        "R-squared: 0.0194807952858\n",
        "BIC 1079.96219957\n",
        "Prior 0.923916982283\n",
        "Posterior 2.84984330375e-235\n",
        " \n",
        "Visits: 3\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.\n",
        "  0.  1.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 19.04724059   0.33401911  -0.40791933   0.59174547  -0.62569581]\n",
        "R-squared: 0.0380013382446\n",
        "BIC 1082.6604525\n",
        "Prior 0.982006264744\n",
        "Posterior 7.85930440924e-236\n",
        " \n",
        "Visits: 3\n",
        "[ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 20.4262238    2.41440696  -0.26245305   0.14688654]\n",
        "R-squared: 0.336572706172\n",
        "BIC 1040.89590316\n",
        "Prior 0.981653444662\n",
        "Posterior 9.21053178985e-227\n",
        " \n",
        "Visits: 3\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.]\n",
        "[10  2  2  2  2  2]\n",
        "[ 19.44777985   0.4549407    0.49975658  -0.71014368]\n",
        "R-squared: 0.032497545191\n",
        "BIC 1078.62577258\n",
        "Prior 0.969668751011\n",
        "Posterior 5.83462388832e-235\n",
        " \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}